#!/bin/bash

#SBATCH --partition=compute
#SBATCH --job-name=metagenome
#SBATCH --mail-type=ALL
#SBATCH --mail-user=katelane@mit.edu
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=180G
#SBATCH --time=48:00:00
#SBATCH --output=metagenome%j.log
#export OMP_NUM_THREADS=36

#Run the setup.sh script first

#Note - This script can be broken into parts in order to run on cluster if jobs must be shorter than 48hrs.

##Create conda environment
conda env create -f LANE_metagenome.yml
conda create -n gtdbtk-1.3.0 -c conda-forge -c bioconda gtdbtk=1.3.0



##Activate conda environment:
conda activate metagenome

#Define rootdir on your computing environment, this is the directory if you type `pwd` inside the directory from cloning this repo. For example, on the Poseidon HPC:
rootdir=/vortexfs1/omics/env-bio/collaboration/jahala/


##Download Data
cd ${rootdir}/data/raw_data/
for i in `cat  access_list_mg.txt`
do
fasterq-dump --split-3 --verbose -O metagenome/ $i
done
#access list is present in /tools/ directory, and in /data/raw_data/ directory. Accession list is also here: SRR10411456


##Trim Reads
cd ${rootdir}/data/raw_data/metagenome

for i in `cat ../access_list_mg.txt`
do
trimmomatic PE -threads 36 ${i}_1.fastq  ${i}_2.fastq ${i}_1.trimmed.fastq.gz ${i}_1un.trimmed.fastq.gz ${i}_2.trimmed.fastq.gz ${i}_2un.trimmed.fastq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 MINLEN:75
done
#un is unpaired output, TruSeq3-PE.fa is present in the raw_data/metagenome/ directory and in the the /tools/ directory for reference

##Filter and Remove Diatom Reads
bowtie2 -x bt2/diatom.fa -1 /vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra/SRR10411456_1.trimmed.fastq.gz -2 /vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra/SRR10411456_2.trimmed.fastq.gz --un-conc bowtie_metagenome_host_removed
(jahala_clean_metagenome) [katherine.lane@poseidon-l2 sra]$ mv bowtie_metagenome_host_removed.2 bowtie_metagenome_host_removed.2.fq
(jahala_clean_metagenome) [katherine.lane@poseidon-l2 sra]$ mv bowtie_metagenome_host_removed.1 bowtie_metagenome_host_removed.1.fq

##Determine Taxonomic Composition of Metagenome
#kaiju requires greater than 24 hrs of computation, as the input was not needed for downstream analyses
Kaiju
wget https://kaiju.binf.ku.dk/database/kaiju_db_refseq_2021-02-26.tgz
(python_lab) [katherine.lane@poseidon-l2 tools]$ tar -xvzf kaiju_db_refseq_2021-02-26.tgz
tool=/vortexfs1/omics/env-bio/collaboration/jahala/tools
dir=/vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra

kaiju -t ${tool}/nodes.dmp -f ${tool}/kaiju_db_refseq.fmi -i ${dir}/bowtie_metagenome_host_removed.1.fq -j ${dir}/bowtie_metagenome_host_removed.2.fq
#could include further instructions on kaiju


##Assembly
megahit
megahit -t 36 -o megahit_assembly2 -1 bowtie_metagenome_host_removed.1.fq -2 bowtie_metagenome_host_removed.2.fq

##Calculate Coverage
#map reads to assembly
dir=/vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra
bwa index ${dir}/megahit_assembly2/final.contigs.fa
bwa mem -t 48 ${dir}/megahit_assembly2/final.contigs.fa ${dir}/bowtie_metagenome_host_removed.1.fq ${dir}/bowtie_metagenome_host_removed.2.fq > aln-pe.sam
samtools sort -@48 aln-pe.sam -o aln-pe.sorted.bam


##Bin Genomes
metabat2
jgi_summarize_bam_contig_depths --outputDepth depth.txt aln-pe.sorted.bam
metabat2 -i final.contigs.fa -a depth.txt -o bins_dir/bin

##Assess MAG Completeness and Quality
#to reduce redundancy in conda environments, use the checkm install in the MAGS environment
conda deactivate
conda activate MAGS

cd ${rootdir}/d]
checkm lineage_wf --pplacer_threads 1 --tab_table -t 16 -x fna ${rootdir}/data/raw_data/MAGS/ ${rootdir}/data/raw_data/MAGS/checkm_results
 tail -17 checkm*.log | head -15 > checkm_stats.tsv
 
#Add bin to scaffold names
for i in *fa; do b=`basename $i .fa`; sed "s/>/>${b}\ /g" $i > ${i}.renamed; done
  
/vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra/
  
#Assess MAG Taxonomy
for i in *renamed; do  sed 's/ /_/g' $i > ${i}.clean; done
mkdir gtdbk_input
 for i in `ls`
> do
> b=`basename $i .fa.renamed.clean`
> mv $i ${b}.fasta
> done

gtdbtk classify_wf --pplacer_cpus 2 -x clean --genome_dir /vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra/megahit_assembly2/bins_dir --out_dir /vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra/megahit_assembly2/bins_dir/gtdbk_out2

(gtdbtk-1.3.0) [katherine.lane@poseidon-l2 gtdbk_out3]$ cut -d ';' -f 5 gtdbtk.bac120.summary.tsv | tail -12 | paste -d '\t' temp.tsv - > bin_taxonomy.tsv

(gtdbtk-1.3.0) [katherine.lane@poseidon-l2 gtdbk_out3]$ cut -d ';' -f 5 gtdbtk.bac120.summary.tsv | tail -12 | sed 's/f__//g' | paste -d '\t' temp.tsv - > bin_taxonomy.tsv
/vortexfs1/omics/env-bio/collaboration/jahala/data/raw_data/metagenome/sra/megahit_assembly2/bins_dir/gtdbk_out3


#Calculate Depth of Coverage for each bin
for i in *fa; do b=`basename $i .fa`; grep '>' $i | sed 's/>//g' >> ${i}.seqs; done
for i in `cat bin.1.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '      ' >> bin_1_counts.txt; done 
for i in `cat bin.2.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_2_counts.txt; done
for i in `cat bin.3.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_3_counts.txt; done                                                                                                                                      
for i in `cat bin.4.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_4_counts.txt; done
for i in `cat bin.5.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_5_counts.txt; done                                                                                                                                       for i in `cat bin.6.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_6_counts.txt; done                                                                                                                                       for i in `cat bin.7.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_7_counts.txt; done                                                                                                                                       for i in `cat bin.8.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_8_counts.txt; done
for i in `cat bin.9.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_9_counts.txt; done
for i in `cat bin.10.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '      ' >> bin_10_counts.txt; done
for i in `cat bin.11.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '      ' >> bin_11_counts.txt; done
for i in `cat bin.12.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '      ' >> bin_12_counts.txt; done
for i in `cat bin.13.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '      ' >> bin_13_counts.txt; done
for i in `cat bin.14.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '      ' >> bin_14_counts.txt; done
for i in `cat bin.15.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '      ' >> bin_15_counts.txt; done
for i in bin_*counts.txt; do awk '{s+=$1} END {print FILENAME "\t" s}' $i >> total_bin_counts.tsv; done
sed -i 's/_counts.txt//g' total_bin_counts.tsv

#total_bin_counts.tsv is the input for metagenome abundance part of Figure 1A

#Annotate Bins
#Performed according to the methods. Here we did not use genes in downstream analysis, as they are not needed for humann3, however they may be useful in manual GO ontology enrichment.
Prokka
