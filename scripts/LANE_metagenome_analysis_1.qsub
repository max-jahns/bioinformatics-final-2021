#!/bin/bash

#SBATCH --partition=compute
#SBATCH --job-name=metagenome
#SBATCH --mail-type=ALL
#SBATCH --mail-user=[YOUR EMAIL]@mit.edu
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=180G
#SBATCH --time=48:00:00
#SBATCH --output=metagenome_%j.log
#export OMP_NUM_THREADS=36

#Run the setup.sh script first

#Note - This script can be subdivided into parts in order to run on cluster if jobs must be shorter than 48hrs.

##Create conda environment
conda env create -f LANE_metagenome.yml

##Activate conda environment:
conda activate metagenome

#Define rootdir on your computing environment, this is the directory if you type `pwd` inside the directory from cloning this repo. For example, on the Poseidon HPC:
rootdir=/vortexfs1/omics/env-bio/collaboration/jahala/
#if your slurm cluster needs hard-coded paths, put ${rootdir} in front of every file path here and this sript will run and produce results


##Download Data
for i in `cat  /tools/access_list_mg.txt`
do
fasterq-dump --split-3 --verbose -O data/metagenome/sra $i
done
#access list is present in /tools/ directory, and in /data/raw_data/ directory. Accession list is also here: SRR10411456


##Trim Reads
for i in `cat /tools/access_list_mg.txt`
do
trimmomatic PE -threads 36 /data/metagenome/sra/${i}_1.fastq  /data/metagenome/sra/${i}_2.fastq /output/metagenome/trim_reads/${i}_1.trimmed.fastq.gz /output/metagenome/trim_reads/${i}_1un.trimmed.fastq.gz /output/metagenome/trim_reads/${i}_2.trimmed.fastq.gz /output/metagenome/trim_reads/${i}_2un.trimmed.fastq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 MINLEN:75
done
#un is unpaired output, TruSeq3-PE.fa is present in the the /tools/ directory for reference


##Filter and Remove Diatom Reads
cd /output/metagenome/filter_host
bowtie2 -x bt2/diatom.fa -1 /output/metagenome/trim_reads/SRR10411456_1.trimmed.fastq.gz -2 /output/metagenome/trim_reads/SRR10411456_2.trimmed.fastq.gz --un-conc bowtie_metagenome_host_removed
mv bowtie_metagenome_host_removed.2 bowtie_metagenome_host_removed.2.fq
mv bowtie_metagenome_host_removed.1 bowtie_metagenome_host_removed.1.fq

##Determine Taxonomic Composition of Metagenome
#With Kaiju first, however this required greater than 24 hrs of computation and large amounts of memory, abundance was calculated on mapping reads back to bins (see below)

tool=/tools/kaiju/
dir=/metagenome/filter_host/

cd /tools/kaiju/
wget https://kaiju.binf.ku.dk/database/kaiju_db_refseq_2021-02-26.tgz
tar -xvzf kaiju_db_refseq_2021-02-26.tgz

cd /output/metagenome/kaiju/
kaiju -t ${tool}/nodes.dmp -f ${tool}/kaiju_db_refseq.fmi -i ${dir}/bowtie_metagenome_host_removed.1.fq -j ${dir}/bowtie_metagenome_host_removed.2.fq


##Assembly
megahit -t 36 -o output/metagenome/megahit_assembly -1 output/metagenome/filter_host/bowtie_metagenome_host_removed.1.fq -2 output/metagenome/filter_host/bowtie_metagenome_host_removed.2.fq

##Calculate Coverage
#map reads to assembly

bwa index /output/metagenome/megahit_assembly/final.contigs.fa
bwa mem -t 48 /output/metagenome/megahit_assembly/final.contigs.fa /output/metagenome/filter_host/bowtie_metagenome_host_removed.1.fq /output/metagenome/filter_host/bowtie_metagenome_host_removed.2.fq > /output/metagenome/metabat/aln-pe.sam
samtools sort -@48 aln-pe.sam -o aln-pe.sorted.bam


##Bin Genomes
jgi_summarize_bam_contig_depths --outputDepth /output/metagenome/metabat/depth.txt /output/metagenome/metabat/aln-pe.sorted.bam
metabat2 -i /output/metagenome/megahit_assembly/final.contigs.fa -a /output/metagenome/metabat/depth.txt -o /output/metagenome/metabat/bins_dir/bin

##Assess MAG Completeness and Quality
#to reduce redundancy in conda environments, use the checkm install in the MAGS environment
conda deactivate
conda activate MAGS

checkm lineage_wf --pplacer_threads 1 --tab_table -t 16 -x fna /output/metagenome/metabat/bins_dir/ /output/metagenome/checkm/checkm_results

#make tsv for genome comparison table
tail -17 /output/metagenome/checkm/checkm_results/checkm*.log | head -15 > /output/metagenome/checkm/checkm_stats.tsv
 
#Add bin to scaffold names and remove space
cd /output/metagenome/metabat/bins_dir/
for i in *fa; do b=`basename $i .fa`; sed "s/>/>${b}\ /g" $i > ${i}.renamed; done
for i in *renamed; do  sed 's/ /_/g' $i > ${i}.clean; done
for i in *clean; do b=`basename = $i .renamed.clean`; mv $i $b; done

  
#back to metagenome conda environment
conda deactivate
conda activate metagenome


#Assess MAG Taxonomy
gtdbtk classify_wf --pplacer_cpus 2 -x fa --genome_dir /output/metagenome/metabat/bins_dir --out_dir /output/metagenome/gtdbk/

#format output for genome taxonomy csv
cut -d ';' -f 5 gtdbtk.bac120.summary.tsv | tail -12 | sed 's/f__//g' | paste -d '\t' temp.tsv - > bin_taxonomy.tsv


#Calculate Depth of Coverage for each bin
cd /output/metagenome/metabat/bins_dir
for i in *fa; do b=`basename $i .fa`; grep '>' $i | sed 's/>//g' >> ${i}.seqs; done
for i in `cat bin.1.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '      ' >> bin_1_counts.txt; done 
for i in `cat bin.2.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '       ' >> bin_2_counts.txt; done
for i in `cat bin.3.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '       ' >> bin_3_counts.txt; done                                                                                                                                      
for i in `cat bin.4.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '       ' >> bin_4_counts.txt; done
for i in `cat bin.5.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '       ' >> bin_5_counts.txt; done                                                                                                                                       for i in `cat bin.6.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_6_counts.txt; done                                                                                                                                       for i in `cat bin.7.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_7_counts.txt; done                                                                                                                                       for i in `cat bin.8.fa.seqs`; do grep $i ../depth.txt | cut -f 3 -d '       ' >> bin_8_counts.txt; done
for i in `cat bin.9.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '       ' >> bin_9_counts.txt; done
for i in `cat bin.10.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '      ' >> bin_10_counts.txt; done
for i in `cat bin.11.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '      ' >> bin_11_counts.txt; done
for i in `cat bin.12.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '      ' >> bin_12_counts.txt; done
for i in `cat bin.13.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '      ' >> bin_13_counts.txt; done
for i in `cat bin.14.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '      ' >> bin_14_counts.txt; done
for i in `cat bin.15.fa.seqs`; do grep $i /output/metagenome/metabat/depth.txt | cut -f 3 -d '      ' >> bin_15_counts.txt; done
for i in bin_*counts.txt; do awk '{s+=$1} END {print FILENAME "\t" s}' $i >> total_bin_counts.tsv; done
sed -i 's/_counts.txt//g' total_bin_counts.tsv

#total_bin_counts.tsv is the input for metagenome abundance part of Figure 1A

#Annotate Bins
for i in /output/metagenome/metabat/bins_dir/*fa
do
b=`basename $i .fa`
cp $i output/metagenome/prokka_bins_dir/${b}.fasta
prokka --locustag $b --outdir $b --prefix $b output/metagenome/prokka_bins_dir/${b}.fasta
